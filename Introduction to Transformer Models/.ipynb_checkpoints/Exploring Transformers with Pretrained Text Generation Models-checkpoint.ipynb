{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0be661d-fe9e-4d95-8193-758f949ce9e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Objective\r\n",
    "Get hands-on experience with Transformer models by generating text using a pretrained model like GPT-2. This project introduces you to the core ideas behind Transformer-based architectures and helps you explore how these models generate coherent, contextually relevant text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce6b7c-bea2-44a6-92eb-648a81f8b9b4",
   "metadata": {},
   "source": [
    "# Project Details\n",
    "\n",
    "You will use a pretrained Transformer model (e.g., GPT-2) within a Jupyter Notebook environment to:\n",
    "\n",
    "- Generate text based on different prompts.\n",
    "\n",
    "- Experiment with generation parameters.\r\n",
    "\n",
    "\n",
    "Reflect on how Transformers understand and extend language sequences.s.uences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaefbe99-47e7-4080-a9e0-6cdb81aadb6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: torch in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (0.33.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leblu\\downloads\\anaconda\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n",
    "!pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b51bcec6-f6de-4e44-8e5e-5621fa5459d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b77b617-b236-4dcf-a0a0-0a57a42c11be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the future, education will be used to improve the quality of life of Canadians, to build and maintain a strong economy, to promote healthy living, and to promote responsible transportation.\n",
      "\n",
      "We have a responsibility to make sure that Canadians have the tools that will enable them to become self-sufficient, and to ensure that the quality of life of our country is not just increased, but increased for the better.\n",
      "\n",
      "To further this goal, we're offering a new program called the Canadian Job Creation Program. As part of the\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load GPT-2 text generation model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Set your prompt\n",
    "prompt = \"In the future, education will\"\n",
    "\n",
    "# Generate text\n",
    "result = generator(prompt,  max_new_tokens=100, temperature=0.7)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e3e446f-0191-4322-9f21-ffc45873c3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The impact of AI on the future of work is very real.\n",
      "\n",
      "A lot of researchers are thinking about what the future might look like. The first big research question was how would AI drive innovation and innovation in the field of medicine? Well, this has gone around and it was always that way. As long as you had that type of data that you wanted, the problem of how to make progress is basically gone.\n",
      "\n",
      "[Laughs.]\n",
      "\n",
      "I think it's safe to say that the most important thing that we don't want\n"
     ]
    }
   ],
   "source": [
    "# Experiment with different prompts\n",
    "prompt = 'The impact of AI on the future of work'\n",
    "result = generator(prompt, max_new_tokens=100, temperature=0.8)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d8d057-ef95-4ac7-b7b9-9f0755c99a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a kingdom of the heavens, and a throne of the earth; and they were like unto the king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth; and they were like unto the king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth, and a king of the earth\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Once upon a time, there was a kingdom'\n",
    "result = generator(prompt,  max_new_tokens=200, temperature=0.6)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef41eb7-a570-4f6e-9d07-c4dab20ee9dd",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "Transformers are a type of neural network architecture that has revolutionized the field of artificial intelligence, particularly in natural language processing (NLP). Unlike traditional models such as RNNs or LSTMs, transformers use a mechanism called self-attention to weigh the importance of each word in a sentence, allowing them to capture long-range dependencies more effectively. This architecture is the foundation for many modern language models, including GPT-2, BERT, and GPT-3. Their ability to handle vast amounts of text and generate coherent responses makes them essential tools in AI today.\n",
    "\n",
    "For this experiment, I used the GPT-2 model provided by Hugging Face’s transformers library along with the pipeline API for text generation. GPT-2 is a large language model pre-trained on internet text and fine-tuned for various downstream tasks. I tested the model in a Python environment and experimented with different prompts and temperature settings to see how the output changed.\n",
    "\n",
    "The prompts I tried included \"In the future, education will\", \"The impact of AI on the future of work\", \"Once upon a time, there was a kingdom\". Each prompt was tested with variations in temperature (e.g., 0.6, 0.7, 0.8) and max_new_tokens (e.g., 100, 200) to observe how the model’s creativity and coherence changed.\n",
    "\n",
    "With the prompt \"In the future, education will\" and a temperature of 0.7, the output was relatively formal and focused on educational reform and inclusivity. It stayed on-topic and mentioned that education will be used to improve the quality of life of Canadians.\n",
    "\n",
    "For \"The impact of AI on the future of work\" with a temperature of 0.8, the response became more expansive and opinionated. It included more abstract thoughts about how AI would drive innovation. The higher temperature led to slightly more diverse and creative text.\n",
    "\n",
    "The prompt \"Once upon a time, there was a kingdom\" with a max_new_tokens of 200 and temperature 0.6 generated a repetitive narrative. It repeated phrases like \"a king of the earth,\" which indicated a limitation in the model’s output control and coherence for longer generations.\n",
    "\n",
    "Adjusting the temperature clearly influenced how creative or predictable the model was. Lower values led to safer, more structured responses, while higher values introduced more surprising or imaginative continuations. Changing the prompt type also affected the domain and tone of the output. Formal prompts generated formal writing, while story-like prompts led to narrative or fairy tale-style continuations.\n",
    "\n",
    "This experiment highlighted the powerful capabilities of transformer-based models like GPT-2 in generating coherent and contextually relevant text from a simple prompt. I learned how even small changes in input such as prompt wording or temperature can dramatically affect the tone, structure, and direction of the generated output.\n",
    "\n",
    "Besides, I noticed that the model sometimes produced responses that appeared to be incomplete or abruptly cut off. For example, when using the prompt \"In the future, education will\", the output ended mid-sentence, suggesting that the model stopped generating before reaching a natural conclusion. This behavior is due to the way GPT-2 handles output length during generation.\n",
    "\n",
    "Specifically, the parameter max_new_tokens controls the maximum number of new tokens (word pieces) the model is allowed to generate. In my case, the generation stopped after reaching this token limit. While GPT-2 has an end-of-text token (with an ID of 50256), it does not always generate this token naturally unless specifically trained to do so or forced with stopping criteria. As a result, even though the model continued generating up to the limit, it did not include a natural stopping point, leading to a response that felt abruptly cut off.\n",
    "\n",
    "This experience highlighted an important limitation of open-ended text generation using transformers: without careful tuning of parameters or manual post-processing (e.g., truncating at sentence boundaries), the output may not always be semantically complete. It reinforced the value of understanding how generation parameters like max_new_tokens, temperature, and token limits interact to shape the final result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
